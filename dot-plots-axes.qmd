---
title: "Axis Limits Provide Context Which Influences Magnitude Judgments"

knitr:
  opts_chunk: 
    cache_comments: false
    
execute:
  echo: false
  warning: false
  message: false
  include: false
  cache: true
    
format:
  ieee-tvcg-pdf: default
  ieee-tvcg-html: default

author:
  - name: Duncan Bradley
    affiliations:
      - name: The University of Manchester
        department: Division of Psychology, Communication, and Human Neuroscience
        address: Oxford Road
        city: Manchester
        country: UK
        postal-code: M13 9PL
    orcid: 0000-0001-7328-8779 
    email: duncan.bradley@manchester.ac.uk
  - name: Gabriel Strain
    affiliations:
      - name: The University of Manchester
        department: Department of Computer Science
        address: Oxford Road
        city: Manchester
        country: UK
        postal-code: M13 9PL
    orcid: 0000-0002-4769-9221
  - name: Caroline Jay
    affiliations:
      - name: The University of Manchester
        department: Department of Computer Science
        address: Oxford Road
        city: Manchester
        country: UK
        postal-code: M13 9PL
    orcid: 0000-0002-6080-1382
  - name: Andrew J. Stewart
    affiliations:
      - name: The University of Manchester
        department: Department of Computer Science
        address: Oxford Road
        city: Manchester
        country: UK
        postal-code: M13 9PL
    orcid: 0000-0002-9795-4104 
      
abstract: |
  When visualizing data, chart designers have the freedom to choose the upper and lower limits of their numerical axes. Axis limits determine the physical positions of plotted values, and can also provide visual context, supporting comprehension. Across two experiments (total N=300), we demonstrate that axis limits affect viewers' interpretations of plotted values' magnitudes {in charts presenting data on the chance of negative events occuring}. Participants did not simply associate values presented at higher vertical positions with greater magnitudes {probabilities}. Instead, they assessed the magnitude of data points by considering them in the numerical context supplied by axis limits. Data points were considered larger when they were closer to the end of the axis associated with greater values, even when they were presented at the *bottom* of a chart. Chart designers must consider the role of their axis limits in viewers' interpretations of the magnitudes of plotted data points. We recommend displaying the range of relevant values in order to communicate the specific context for each dataset.
keywords: "magnitude, axis range, dot plots, bias, framing effect, context, position"
bibliography: bibliography.bib
link_citations: true

teaser:
  image: images/CypressView
  caption: | 
  
    .

ieee-vgtc-metadata:
  online-id: "0"
  # please declare the paper type of your paper to help reviewers, only shown in review mode
  # choices:
  # * algorithm/technique
  # * application/design study
  # * evaluation
  # * system
  # * theory/model
  paper-type: please specify
  short-author-title: "Bradley \\MakeLowercase{\\textit{et al.}}: short title"
  keywords: ""
---

```{r}
#| label: setup
#| cache: false
set.seed(45789) # seed for random number generation

# Loading packages
library(papaja) 
library(tidyverse) 
library(ordinal) 
library(patchwork)
library(magick) 
library(emmeans) 
library(egg) 
library(scales) 
library(buildmer) 
library(lme4)
library(broom)
library(insight)
library(qwraps2)
library(kableExtra)
library(effectsize)
```

```{r}
#| label: load-data
#| cache: false
# loading data
# see anonymisation.R for the script used to filter rejected participants and remove Prolific IDs
E1_anon <- read_csv("data/E1_anon.csv")
E2_anon <- read_csv("data/E2_anon.csv")
```

```{r}
#| label: wrangle
#| cache: false
# perform necessary data wrangling
wrangle <- function(anon_file, .y) {
  # .y captures the index of the file in the list supplied to iwalk

# extract literacy data
# calculate literacy score (sum of five responses)
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)

# define education categories 
edu_labels <- set_names(c('No formal qualications',
                'Secondary education (e.g. GED/GCSE)',
                'High school diploma/A-levels',
                'Technical/community college',
                'Undergraduate degree (BA/BSc/other)',
                'Graduate degree (MA/MSc/MPhil/other)',
                'Doctorate degree (PhD/other)',
                'Don\'t know / not applicable'),
          seq(8,1,-1))

# extract demographics
# link slider response numbers to gender categories 
# link slider response numbers to education categories
  demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  mutate(gender_slider.response = recode(gender_slider.response, 
                                         `1` = "F", 
                                         `2` = "M", 
                                         `3` = "NB")) %>%
  mutate(across(matches("edu_slider.response"),
                ~recode(edu_slider.response, !!!edu_labels))) %>%
      select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response",
                   "edu_slider.response")))
  
# select relevant columns
# select only experimental items
# add literacy and demographic data
# change data types where appropriate
# output this file with suffix 'tidy'
  anon_file %>% 
  select(matches(c("participant", 
                   "item_no",
                   "condition",
                   "chance_slider.response",
                   "severity_slider.response",
                   "chance_slider.rt",
                   "severity_slider.rt",
                   "data_mean",
                   "key_resp.rt",
                   "type",
                   "time_taken"))) %>% 
    filter(type == "E") %>%
    inner_join(literacy, by = "participant") %>%
    inner_join(demographics, by = "participant") %>%
    mutate(across(matches(c("condition", "list_number")), as_factor)) %>%
    mutate(across(c("chance_slider.response",
                  "severity_slider.response"), as.ordered)) %>%
    mutate(across(c("participant",
                  "item_no"), as.character)) %>%
    mutate(time_taken = time_taken / 60) %>%
    assign(paste0("E", .y, "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use 'wrangle' function defined above on each data file
iwalk(list(E1_anon, E2_anon), wrangle)

# set contrasts to facilitate statistical analysis
contrasts(E1_tidy$condition) <- matrix(c(.5, -.5))
contrasts(E2_tidy$condition) <- matrix(c(.5, -.5))
```

```{r}
#| label: comparison-function
#| cache: false
# this function takes a model and creates a nested model with one fixed effects term removed, for anova comparison
# the default is to remove the last fixed effects term
# but a particular term can be specified use 'remove = '
comparison <- function(model, remove = NULL) {
  
  form <- formula(model)
  
  reducefixed <- function(form) {
    
    fixedfx <- 
      remove.terms(form,"placeholder") %>% # generate full formula (expand '*')
      nobars() # get formula for fixed effects only
    
    fixedterms <-  
      terms.formula(fixedfx) %>% # get terms for fixed effects
      attr("term.labels") # get character vector of fixed effects terms
    
    out <- remove.terms(fixedfx, tail(fixedterms, n=1))
    
    # remove will only take a single character string, not a character vector
    if(!is.null(remove))
      out <- remove.terms(fixedfx, remove)
    
    return(out)
  }
  
  getrandom <- function(form) {
    
    parens <- function(x) {paste0("(",x,")")}
    onlyBars <- function(form) {
      reformulate(
        sapply(
          findbars(form), # list of character vector for each random effect
          function(x)  parens(deparse(x))), # put each character vector in brackets
        response = form[[2]]) 
    }
    
    out <- onlyBars(form)
    return(out)
  }
  
  merge.formula <- function(form1, form2, ...){
    # adapted from https://stevencarlislewalker.wordpress.com/2012/08/06/merging-combining-adding-together-two-formula-objects-in-r/
    
    # get character strings of the names for the responses 
    # (i.e. left hand sides, lhs)
    lhs1 <- deparse(form1[[2]])
    #print(lhs1)
    lhs2 <- deparse(form2[[2]])
    #print(lhs2)
    if(lhs1 != lhs2) stop('both formulas must have the same response')
    
    # get character strings of the right hand sides
    rhs1 <- strsplit(paste(form1[3]), " \\+ ")[[1]] 
    rhs2 <- strsplit(paste(form2[3]), " \\+ ")[[1]] 
    
    # put the two sides together with the amazing 
    # reformulate function
    out <- reformulate(termlabels = c(rhs1, rhs2), 
                       response = lhs1)
    
    # set the environment of the formula (i.e. where should
    # R look for variables when data aren't specified?)
    #environment(out) <- parent.frame()
    return(out)
  }
  
  newfixedfx <- reducefixed(form)
  fullranfx <- getrandom(form)
  merge.formula(newfixedfx, fullranfx)
  
}
```

```{r}
#| label: anova-results-function
#| cache: false
# this function takes two nested models, runs an anova, and the outputs the Likelihood Ratio Statistic, degrees of freedom, and p value to the global environment
anova_results <- function(model, cmpr_model) {
  
  # first argument 
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
      
  anova_output <- ordinal:::anova.clm(model, cmpr_model)
  # use of ordinal:::anova.clm based on https://github.com/runehaubo/ordinal/issues/38  
  
  assign(paste0(model_name, ".LR"),
         anova_output$LR.stat[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
  # es <- eta_squared(model) 
  # 
  # es %>% pull(Parameter) %>%
  #   map(function(x) assign(paste0(model_name, 
  #                                 ".eta.", 
  #                                 str_replace(x, ":", "_")),
  #                          es %>%
  #                            filter(Parameter == x) %>% 
  #                            pull(Eta2_partial),
  #                          envir = .GlobalEnv))
}
```

```{r}
#| label: summary-extract-function
#| cache: false
# this function extracts test statistics and p values from model summaries
summary_extract <- function(model, key_term) {
  
  params <- c("statistic", "p.value", "estimate")

  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  # get the row for the chosen fixed effect term
  one_row <- tidy(model) %>% filter(term == key_term)

    get_cols <- function(param) {

    assign(value = one_row %>% pull(param),
           envir = .GlobalEnv,
           paste0(model_name, ".", param))
    }

    lapply(params, get_cols)
    
    
    assign(value = confint(model)[key_term,],
           envir = .GlobalEnv,
           paste0(model_name, ".CI"))

}
```

```{r}
#| label: random-str-function
#| cache: false
# this function creates a table which displays the random effects structure (intercepts and slopes) for a given model
random_str <- function(model) {
  model <- model@model
  terms <- model %>% find_random %>% unlist() %>% unname()
  mylist <- model %>% formula %>% findbars() %>% as.character()
  slopes <- lapply(mylist, str_extract, "(?<=\\+ )(.*)(?= \\| )") %>% 
    unlist()
  tibble(terms, slopes)
}
```

# Introduction

Context is crucial for effectively judging the magnitude of numbers. A 40% probability is twice as great as a 20% probability, but in the absence of context, it is unclear whether this value should be considered large or small. For the chance of experiencing post-surgery complications, a 40% probability may be considered large, but for the chance that a laboratory test can detect a disease, a 40% probability may be considered small.

```{r}
#| label: fig-senators-chart
#| cache: false
#| include: true
#| out.width: "300px"
#| out.height: "210px"
#| fig.asp: 0.7
#| fig-cap: A reproduction of a data visualisation from the New York Times. The y-axis limit is the largest possible value, rather than the largest observed value. The present study asks whether magnitude appears small because the plotted values occupy a low position, or because the plotted values are presented in the context of much larger values.

read_csv("images/senators.csv") %>%
  ggplot(aes(x = year)) +  
  geom_histogram(binwidth = 1, fill = "black") +
  labs(x = NULL,
       y = NULL,
       caption = expression(paste("Source: Office of the Historian, ", italic("History, Art & Archives, U.S. House of Representatives")))) +
  annotate("text", x=1905, y=75, 
           label="Number of Black members of the U.S. Senate",
           size = 5) +
  scale_x_continuous(breaks = seq(1800, 
                                  2020,
                                  by = 20),
                     labels = seq(1800, 
                                  2020,
                                  by = 20)) +
  scale_y_continuous(breaks = seq(0, 
                                  100,
                                  by = 10)) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(colour = "lightgrey"),
        axis.ticks.y = element_blank(),
        axis.text.y = element_text(size = 15),
        axis.text.x = element_text(size = 11),
        panel.background = element_blank()) +
  coord_cartesian(ylim = c(0, 100), 
                  xlim = c(1789, 2024),
                  clip = "off",
                  expand = FALSE) + 
  geom_hline(yintercept = 0)
```

Contextual cues may influence interpretation of magnitude in data visualizations. One such cue is the range of values on an axis, which can serve as a frame of reference for assessing whether a data point is numerically high or low. @fig-senators-chart (a reproduction of a similar bar chart from the New York Times), which plots over time the number of Black members of the U.S. senate \[ref\], provides a striking illustration. Unusually, the continuous y-axis does not terminate just above the highest plotted value. Instead, it extends all the way to the maximum possible number of senators: 100. As a result, bars representing Black senators are confined to the very bottom, visible just above the x-axis, and a significant expanse of blank space looms above them. This highlights the absent data points: the vast majority of senators who are not Black. The chart communicates the magnitude of the plotted values in context.

It is unclear exactly how a viewer's inferences about magnitude might be influenced by axis range. Different axis limits will present the data points at different positions, so one possible explanation is that viewers may interpret the magnitude of data points at higher positions as 'high' and those at lower positions as 'low'. Alternatively, the axis may indicate a range of relevant values: plotted values may be judged as small in magnitude when the potential for larger values is clearly displayed. The present pair of experiments demonstrates the influence of axis limits on interpretations and explores which of these two accounts best explains how axis limits contribute to the communication of magnitude.

##### Overview

In two experiments, we manipulated the axis limits surrounding plotted data. The same data points either appeared close to the upper end of an axis range, or close to the lower end. Likert scale ratings of values' magnitudes were higher when data points were positioned close to the end of the axis which was associated with higher numbers. By employing both charts with conventional and inverted orientations to distinguish between possible explanations, we reveal that magnitude judgments are influenced by data points' relative position within the range of axis values.

# Related Work

## Effects of Axis Limits on Comparison Judgments

Several studies have explored the role of axis limits in data visualization. This work has typically focused on how axis limits can alter impressions of the *difference between* presented values. For example, when axis ranges are expanded to create blank space around a cluster of data points, correlation between those points is judged as stronger @cleveland1982. Participants rate the differences between values in bar charts as greater when the vertical gap between bars is larger, due to a truncated y-axis .

Correll et al.'s [@correll2020] experiments found that greater truncation resulted in higher effect-size judgments in both line charts and bar charts. They found no reduction in effect size judgments when truncation was communicated using graphical techniques (e.g., axis breaks and gradients). Truncation effects persisted even when participants estimated the values of specific data points. This suggests the bias is driven by initial impressions, rather than a misinterpretation of the values portrayed by graphical markings. The unavoidable consequence, Correll et al. suggest, is that designers' choices will influence viewers' interpretations whether axes are truncated or not.

Choosing an appropriate axis range involves a trade-off between participants' bias (over-reliance on the visual appearance of differences) and their sensitivity (capacity to identify actual differences) [@witt2019] . Just as a highly truncated y-axis can exaggerate trivial differences between values, an axis spanning the entire range of possible values can conceal important differences. Based on participants' judgments of effect size, @witt2019 found that bias was reduced and sensitivity increased when using an axis range of approximately 1.5 standard deviations of the plotted data, compared to axes which spanned only the range of the data, or the full range of possible values. This provides further evidence of a powerful association between the appearance of data, when plotted, and subjective interpretations of differences between data points.

Further evidence of truncation effects, provided by @yang2021 improves on the design of previous studies which employed only a few observations per condition [@pandey2015] or very small sample sizes [@witt2019]. Participants' ratings of the difference between two bars consistently provided evidence of the exaggerating effects of y-axis truncation. @yang2021 noted that increasing awareness does not eliminate this effect, which may function like an anchoring bias, where numerical judgments are influenced by reference points [@tversky1974]. Another potential explanation discussed draws upon Grice's cooperative principle [@grice1975]. According to this account of effective communication, speakers are assumed to be in cooperation, and so will communicate in a manner that is informative, truthful, relevant, and straightforward. Analogously, a viewer will assume that a numerical difference in a chart must be genuinely large if it appears large, else it would not be presented that way. Effective visualizations should be designed so a viewer's instinctive characterization of the data corresponds closely to their interpretation following a more detailed inspection [@yang2021].

## Effects of Axis Limits on Magnitude Judgments

The above research consistently demonstrates that the magnitude of *the difference between values* is interpreted differently depending on the axis limits employed. The present investigation is concerned with how interpretations of the magnitude of *the values themselves* are affected by a chart's design.

Empirical evidence demonstrates that judgment of a value's magnitude can depend on its relationship to a grand total or to surrounding values. This can influence interpretation of verbal approximations, and also absolute values. For example, participants instructed to take 'a few' marbles picked up more when the total number available was larger [@borges1974] and rated satisfaction with the same salary as higher when it appeared in the upper end of a range, compared to the lower end [@brown2008] . As well as context, vertical position also plays a role in magnitude judgments. For example, children appear to intuitively understand the relationship between height and value [@gattis2002]. Both the physical world, and language (e.g., spatial metaphors), provide countless examples where 'higher' is associated with 'more', and 'lower' with 'less', and this principle has been adopted as a convention in data visualization [@tversky1997].

In charts, inversions of the typical mapping between magnitude and vertical position charts can lead to misinterpretations [@okan2012; @pandey2015; @woodin2022]. Furthermore, when a company's financial performance was displayed entirely in the bottom fifth of a line chart, the company was perceived as less successful compared to when the axis did not extend above the maximum value [@taylor1986] . @sandman1994 investigated assessments of magnitude in risk ladders, where greater risks are presented at physically higher positions on a vertical scale. Participants rated asbestos exposure as a greater threat when it was plotted at a higher position, compared to a lower position.

The above findings can be regarded as preliminary evidence that changing axis limits may affect appraisals of data points' magnitudes. However, the evidence is not substantial. @taylor1986 did not disclose how judgments were elicited, or provide details of their sample size. @sandman1994 only explored responses to one specific risk (asbestos), and each participant only took part in a single trial. Their perceived threat measure was a composite of several separate ratings, preventing diagnosis of whether manipulations affected interpretations of the plotted information in particular, or just related concepts. Further, both studies introduced a confounding variable by adjusting the difference between the minimum and maximum y-axis values across conditions. Stronger evidence is required regarding how axis limits may bias inferences about magnitude, and the cognitive mechanisms involved in generating these inferences.

## Judgments of Risk Magnitude

In the present study, participants viewed charts showing fictitious data on the chance of negative outcomes occurring (i.e., scenarios involving risk). This provided participants with a purpose; evaluating information about risks is a more meaningful task than assessing how 'large' an abstract value is.

We employed simple dot plots to convey percentages, avoiding complications from manipulating charts with additional encoding channels (e.g., bar charts). Recommendations for communicating risks suggest that the uncertainty should be explicitly encoded and discrete outcomes displayed (refs). Representing each estimate with a single dot on a percentage probability scale violates this guidance. Therefore, we do not consider the visualizations in this set of experiments reflective of best practice for visualizing probabilistic information.

Whilst our focus is on communication of numerical information generally, rather than risk specifically, we nonetheless sought to examine cognitive processes related to risk judgments. Risk events are composed of two core components: 1) chance of occurrence and 2) outcome magnitude (severity). Individuals' assessments of chance and severity are not necessarily independent. Events are perceived as more likely when they are described as having more severe consequences [@harris2009; @harris2011]. In a similar manner, events are associated with more substantial consequences when they are described as more likely [@kupor2020].

One account suggests that perceptions of probability and outcome magnitude are related because they are both assumed to reflect the potency of the event's cause (probability-outcome correspondence principle; [@keren2001]. According to this account, probabilities can occasionally provide meaningful indications of outcome magnitude (e.g., rainfall), but it is inappropriate to apply this perspective to all situations (e.g., volcanic eruptions). Therefore, even though charts in the present study only display the *chance* of events occurring, assessments of the *severity* of events' consequences may also differ between conditions. Collecting separate judgments of chance and severity of consequences for each scenario provides a clearer picture of how the manipulation affects distinct aspects of participants' representations of risk. Use of Likert scales (with discrete options) rather than visual analogue scales (with continuous options; [@sung2018]) prevents participants from simply mapping probability percentages directly onto a linear scale.

## Data Visualization Literacy

When faced with charts that violate graphical conventions by using atypical scales, individuals with low literacy are more likely to draw on data points' physical positions when making inferences about their magnitudes [@okan2012; @okan2016]. We administered Garcia-Retamero et al.'s [-@garcia-retamero2016] subjective graph literacy measure to determine whether responses to our manipulation of axis limits were associated with data\]visualization literacy.

# Experiments

We conducted two experiments manipulating y-axis limits in visualizations of fictitious data. This manipulation altered the physical positions of data points in a chart, but crucially the numerical values themselves remained the same.

Experiment 1 sought to establish whether y-axis limits affected magnitude judgments. To provide context for participants, text accompanying the charts outlined (fictitious) scenarios involving a specific risk (e.g., loss on financial investment, delayed flights, etc.). Three plotted data points in each chart represented the chance of the negative outcome occurring (%) for three instances associated with the scenario (e.g., three investment opportunities, three airlines, etc.).

Experiment 2 used the same y-axis manipulation as Experiment 1, but in charts with inverted y-axis orientation, where data points at lower physical positions represented greater values. This allowed us to investigate whether magnitude judgments were driven by data points' absolute positions, or their relative positions within the context of the axis limits. Our use of inverted charts should not be considered an endorsement (see issues above). They serve to distinguish between two possible explanations, since they reverse the typical associations between physical position and magnitude.

Ethical approval was granted by The University of Manchester's Division of Neuroscience & Experimental Psychology Ethics Committee (Experiment 1: Ref 2021-11115-18258; Experiment 2: Ref. 2021-11115-20745).

## Experiment 1

### Method

#### Materials

##### Datasets

For each dataset, we generated three values from a normal distribution. Population means were specified manually in order to represent plausible values for the probability of the event occurring (XX% - XX-%). Population standard deviation was XX for all datasets. The same dataset was employed for both of the experimental conditions associated with a given risk scenario.

##### Charts

```{r}
#| label: fig-example
#| cache: false
#| include: true
#| out-height: "20%"
#| out-width: "300px"
#| fig-asp: 0.6
#| fig.cap: "Example Charts. The 'high physical position' condition (left) presents data points near the top of the chart; the 'low physical position' condition (right) presents the same data points near the bottom of the chart."

# fig-width/height seems to make no difference to the dimensions of this image - is there a way to aut
# combine the two images of the charts in the two separate conditions
img1 <- image_read("images/E23_hi.png")
img2 <- image_read("images/E23_lo.png")
image_append(c(img1, img2))
```

Datasets were displayed using dot plots. In experimental trials (n = 40), upper and lower axis limits were manipulated such that data points either appeared in the top third of the chart (high physical position: @fig-example , left) or in the bottom third (low physical position: @fig-example, right).

The y-axis range in each chart was 10 percentage points. Horizontal gridlines appeared at one-unit increments. The gridlines 1.5 percentages points from the extremes were labelled with numerical values.

Filler trials (n = 15) and attention check trials (n = 5) presented data points in the middle third of the chart. Filler trials employed this additional variation to prevent participants from identifying the purpose of the study.

#### Procedure

The experiment was programmed in PsychoPy (version 2021.1.4, @peirce2019 and hosted on pavlovia.org. Participants were instructed to complete the experiment on a desktop computer or laptop, not a tablet or mobile phone. Instructions explained that their task involved assessing the chance and severity of negative outcomes in various scenarios involving risks and noted that some scenarios might appear similar to other scenarios. Participants were asked to complete the task as quickly and accurately as possible. Two practice trials preceded the experiment proper.

Participants provided two responses in each trial: a rating of the chance of the negative event occurring; and a rating of the severity of the consequences if that negative event occurred. Both 7-point Likert scales had two anchors at their extremes: *'Very unlikely'* and *'Very likely'*; for the 'Chance' scale and *'Very mild'* to *'Very severe'*. for the 'Severity' scale. All other points were unlabeled. Text specified that answers should be given in response to the plotted data (e.g., *"If you camp on one of these days..."*). The term 'chance' was used instead of 'probability' to avoid confusion with the standard 0-1 scale for probabilities, and to reflect casual usage.

Participants could change their responses as many times as they wished before proceeding to the next trial, but could not return to previous trials. In attention check trials, participants were instructed not to attend to the chart, and instead to provide specified responses on the Likert scales.

Before exiting the experiment, participants were informed that all presented data were fictitious and guidance was provided in case of distress.

#### Design

We employed a repeated-measures, within-participants design. In experimental trials, participants encountered each scenario twice: once with data presented at a high physical position and once with data presented at a low physical position.

Materials were divided into two lists to minimize the likelihood of different versions of the same scenario appearing in close succession. One list contained half of the high-condition items and half of the low-condition items for the experimental scenarios. The other list contained the alternate versions of each of the experimental scenarios. Fillers and attention check questions were split between the two lists, and did not appear more than once. The order of the two lists was counterbalanced across participants. Within each list, scenarios were presented in a random order.

#### Participants

The experiments were advertised on Prolific.co, a platform for recruiting participants for online studies. Normal or corrected-to-normal vision and English fluency were required for participation.

```{r}
#| label: E1_demographics
#| cache: false
# extract age data
age_E1 <- distinct(E1_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(age_textbox.text, na.rm = TRUE), sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract gender data
gender_E1 <- distinct(E1_tidy, participant, .keep_all = TRUE) %>% group_by(gender_slider.response) %>% summarise(perc = n()/nrow(.)*100) %>% pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract literacy data
literacy_E1 <- distinct(E1_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(literacy), sd = sd(literacy))

# extract timing data
# timing data was not available for one participant, and was over-estimated for another because submission was manually returned
time_E1 <- distinct(E1_tidy, participant, .keep_all = TRUE) %>% filter(time_taken < 300 & !is.na(time_taken)) %>% summarise(mean = mean(time_taken), sd = sd(time_taken))
```

Data were returned by 160 participants. Ten participants' submissions were rejected because they answered more than two of 10 attention check questions incorrectly. This left a total of 150 participants whose submissions were used for analysis (`r printnum(gender_E1$M)`% male, `r printnum(gender_E1$F)`% female, `r printnum(gender_E1$NB)`% non-binary). Mean age was `r printnum(age_E1$mean)` (*SD* = `r printnum(age_E1$sd)`)[^1]. The mean graph literacy score was `r printnum(literacy_E1$mean)` (*SD* = `r printnum(literacy_E1$sd)`), out of a maximum of 30. Participants whose submissions were approved were paid £3.55. Average completion time was `r printnum(time_E1$mean, digits = 0)` minutes [^2].

[^1]: Age data was unavailable for one participant, but was available for all other participants in the dataset.

[^2]: Timing data was unavailable for two participants, but was available for all other participants in the dataset.

## Analysis Technique

Analyses were conducted using R (version 4.1.2, \[create new reference\]).

Likert scales express granularity at the level of ordinal data. They record whether one rating is higher or lower than than another, but not the magnitude of this difference. Therefore, Likert scales do not capture values from latent distributions (mental representations) in a linear manner. The distance between one pair of points and another pair may appear equal, but may represent different distances on the latent distribution. Therefore, it is inappropriate to analyse Likert scale data with metric models, such as linear regression [@liddell2018]. Throughout this paper, we construct cumulative link mixed-effects models, using the *ordinal* package (version 2019.12-10, [@christensen2019a]) to analyse Likert scale ratings.

Selection of model random effects structures was automated using the *buildmer* package in R (version 2.3, [@voeten2022]). The maximal random effects structure included random intercepts for participants and scenarios, plus corresponding slopes for fixed effects terms [@barr2013]. *buildmer* initially identified the most complex model which could successfully converge. It subsequently removed terms which did not contribute substantially to explaining variance in ratings.

## Results

```{r}
#| label: likert-plot-function
#| cache: false
# this function plots the distribution of responses for ratings of data points' magnitudes (chance of negative outcomes occurring)
likert_plot <- function(df) {
  
  # select name of key IV
  IV1 <- df %>% select(matches(c("condition", "pos"))) %>% names() %>% as.name()
  
  # extract names of the conditions
  conds <- df %>% select(matches(c("condition", "pos", "ori"))) %>% names() 
  
  # get number of observations in each separate condition
  n_obs <- df %>% nrow()/(length(conds)*2)

  df %>% 
    ggplot(aes(y = {{IV1}}, fill = chance_slider.response)) +
    geom_bar(width=0.9,
             position = position_stack(reverse = TRUE)) +
    scale_fill_brewer(type = "div", palette = "RdYlGn",
                      direction = -1) +
    labs(x = NULL,
         y = NULL) +
    scale_y_discrete(labels=c("Low", "High"),
                     limits = c("lo", "hi"))  +
    scale_x_continuous(labels=c("\"Very\nunlikely\"",
                                "\"Very\nlikely\""), 
                       breaks = c(0, n_obs),
                       position = "top") +
    theme_minimal(base_size = 18) +
    theme(axis.text.x = element_text(face = "italic"),
          legend.position = "none",
          panel.grid = element_blank(),
          plot.title = element_text(size=18, hjust = 0.5)) +
    coord_fixed(ratio = n_obs/15, 
                ylim = c(0.5, 2.5), 
                xlim = c(0, n_obs+n_obs/10),
                    clip = "off", expand = FALSE) 
}
```

### Magnitude Ratings

```{r}
#| label: fig-E1-c
#| cache: false
#| include: true
#| out-width: "300px"
#| fig.cap: "Participants rated the chance of each negative event occurring on a 7-point Likert scale. The distribution of ratings, ranging from \"Very unlikely\" (far left, dark green) to \"Very likely\" (far right, red), is shown separately for charts where values were presented at a high physical position (top) and a low physical position (bottom). Note that data points at high physical positions elicited a larger proportion of ratings on the right-hand side (which represents greater magnitudes), compared to data points at low physical positions, which elicited a larger proportion of ratings on the left-hand side (representing smaller magnitudes)."
# create Likert plot for E1 data - chance (magnitude) ratings
L1 <- likert_plot(E1_tidy) +
  annotate("segment", x=400, y=3.2, xend=2700, yend=3.2,
           arrow=arrow(ends = "both", type = "closed", length = unit(10, "pt"))) +
  labs(title = "Experiment 1:\nRatings of Data Points' Magnitudes")

E1_prop <- E1_tidy %>%
  group_by(condition, chance_slider.response) %>%
  summarize(freq = n(),
            raw = round(n()/3000*100, 0),
            perc = paste0(round(n()/3000*100, 0), "%")) %>%
  ungroup() %>%
  filter(raw > 3)

L1 <- L1 + geom_text(
  data=E1_prop,
  aes(x=freq, label=(perc), group=condition),
  position = position_stack(reverse = TRUE, vjust = 0.5),
  color="black", size=3.5
)

L1
```

```{r}
#| label: E1-c
# E1 chance (magnitude) rating model
E1_c <- buildclmm(chance_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = E1_tidy)
```

```{r}
#| label: E1-c-cmpr
# E1 chance (magnitude) rating model without fixed effect
E1_c_cmpr <- clmm(comparison(E1_c),
             data = E1_tidy)
```

```{r}
#| label: E1-c-anova
#| cache: false
# extract anova results for E1 chance (magnitude) rating models
anova_results(E1_c, E1_c_cmpr)

# summary output for E1 chance (magnitude) rating model
summary_extract(E1_c, "condition1")
```

```{r}
#| label: E1-c-str
#| cache: false
#| include: true
# print table showing model random effects structure (not used in final document)
add_header_above(kable(random_str(E1_c), "latex"), "E1_c")
```

@fig-E1-c plots the distribution of participants' ratings of data points' magnitudes, for data points presented at high and low physical positions. A likelihood ratio test reveals that a model including physical position as a fixed effect explains significantly more variability in ratings than a model which does not include physical position as a fixed effect ($\chi^2$(`r in_paren(E1_c.df)`) = `r printnum(E1_c.LR)`, p `r printp(E1_c.p, add_equals = TRUE)`). Data points' magnitudes were rated as greater when those data points were presented at high physical positions, compared to when the same data points were presented at low physical positions (z = `r printnum(abs(E1_c.statistic))`, p `r printp(E1_c.p.value, add_equals = TRUE)`). 

The odds ratio for the difference between conditions is `r printnum(exp(E1_c.estimate))`, `r print_confint(exp(E1_c.CI))`. Participants were `r round(exp(E1_c.estimate), 1)` times more likely to award a higher magnitude rating to data points presented at high positions than data points presented at low positions. This is equivalent to a Cohen's d value of `r printnum(oddsratio_to_d(exp(E1_c.estimate)))`. 

This model employed random intercepts for each scenario and each participant. Estimated marginal means, calculated using the *emmeans* package (version 1.7.0, [@lenth_emmeans_2021]) for these ratings are plotted in @fig-E1-c-emm.

```{r}
#| label: fig-E1-c-emm
#| cache: false
#| include: true
#| out-width: "300px"
#| fig.cap: "Estimated marginal means for ratings of data points' magnitudes (generated by the cumulative-link mixed model). Magnitudes were rated as greater when data points were presented at high physical positions. Translucent bars show 95\\% confidence intervals."
# extract estimated marginal means for E1 chance (magnitude) ratings
E1_c_emm <- emmeans(E1_c@model, ~ condition) %>% as_tibble()

# define my_palette - colour-deficiency friendly
my_palette <- unname(palette.colors(palette = "Okabe-Ito")[2:3])
# select the first colour in this palette for this plot
hex_conventional <- my_palette[1]

# generate plot of estimated marginal means for E1 chance (magnitude) ratings in the two conditions
E1_c_emm %>%
  as_tibble() %>%
  mutate_at(vars("emmean":"asymp.UCL"), as.numeric) %>%
  ggplot(aes(x = condition, y = emmean, colour = I(hex_conventional), group = 1)) +
  geom_linerange(aes(ymin = asymp.LCL, ymax = asymp.UCL),
                 position = position_dodge(width = 0.1),
                 size = 3, alpha = 0.5) +
  geom_point(position = position_dodge(width = 0.1), size = 3) +
  geom_line(position = position_dodge(width = 0.1),
            size = 2) +
  lims(y = c(-1.8, 2)) +
  labs(y = "Estimated\nMarginal Mean",
       x = "Physical Position",
       title = "Experiment 1:\nRatings of Data Points' Magnitudes (Modeled)") +
  scale_x_discrete(labels = c('Low','High'),
                   limits = c("lo", "hi")) +
  theme_minimal(base_size = 18) +
  theme(legend.position = "none",
        panel.grid = element_blank(),
        #axis.text.y = element_blank(),
        plot.title = element_text(size=18, hjust = 0.5))
```

### Severity Ratings

```{r}
#| label: E1-s
# E1 severity rating model
E1_s <- buildclmm(severity_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = E1_tidy)
```

```{r}
#| label: E1-s-cmpr
# E1 severity rating model without fixed effect
E1_s_cmpr <- clmm(comparison(E1_s),
             data = E1_tidy)
```

```{r}
#| label: E1-s-anova
#| cache: false
# extract anova results for E1 severity rating models
anova_results(E1_s, E1_s_cmpr)

# summary output for E1 severity rating model
summary_extract(E1_s, "condition1")
```

```{r}
#| label: E1-s-str
#| cache: false
#| include: true
# print table showing model random effects structure (not used in final document)
add_header_above(kable(random_str(E1_s), "latex"), "E1_s")
```

For ratings of the severity of consequences, a likelihood ratio test reveals that a model including physical position as a fixed effect explains significantly more variability in ratings than a model which does not include condition as a fixed effect: ($\chi^2$(`r in_paren(E1_s.df)`) = `r printnum(E1_s.LR)`, p `r printp(E1_s.p, add_equals = TRUE)`). The severity of consequences was rated as greater when data points representing the chance of an event occurring were presented at high physical positions, compared to when the same data points were presented at low physical positions (z = `r printnum(abs(E1_s.statistic))`, p `r printp(E1_s.p.value, add_equals = TRUE)`). 

The odds ratio for the difference between conditions is `r printnum(exp(E1_s.estimate))`, `r print_confint(exp(E1_s.CI))`. Participants were `r round(exp(E1_s.estimate), 1)` times more likely to award a higher severity rating to data points presented at high positions than data points presented at low positions. This is equivalent to a Cohen's d value of `r printnum(oddsratio_to_d(exp(E1_s.estimate)))`. 

This model employed random intercepts for each scenario, plus random intercepts and slopes for each participant. The slopes modeled, for each participant, the average difference between responses to data presented at different positions (henceforth referred to as 'by-position slopes').

### Data Visualisation Literacy

```{r}
#| label: E1-cl
# generate E1 chance (magnitude) rating model with literacy as additional fixed effect
E1_cl <- clmm(add.terms(formula(E1_c), "literacy"),
              data = E1_tidy)
```

```{r}
#| label: E1-sl
# generate E1 severity rating model with literacy as additional fixed effect
E1_sl <- clmm(add.terms(formula(E1_s), "literacy"),
              data = E1_tidy)
```

```{r}
#| label: E1-lit-summary
#| cache: false
# summary output for E1 chance (magnitude) rating model with literacy
summary_extract(E1_cl, "condition1")

# summary output for E1 severity rating model with literacy
summary_extract(E1_sl, "condition1")
```

We also generated two additional models, to test whether or not the above results could be explained by differences in graph literacy. These models were identical to the above models except for the inclusion of participants' graph literacy scores as an additional fixed effect. Adjusting for participants' graph literacy scores did not eliminate the effects of data points' positions on ratings of the magnitude of data points themselves (z = `r printnum(abs(E1_cl.statistic))`, p `r printp(E1_cl.p.value, add_equals = TRUE)`, odds ratio = `r printnum(exp(E1_cl.estimate))`, `r print_confint(exp(E1_cl.CI))`) or severity of consequences (z = `r printnum(abs(E1_sl.statistic))`, p `r printp(E1_sl.p.value, add_equals = TRUE)`, odds ratio = `r printnum(exp(E1_sl.estimate))`, `r print_confint(exp(E1_sl.CI))`).

## Discussion

# Experiment 2

## Introduction

Experiment 1 (E1) found that participants associated data points with greater magnitudes when those data points were positioned near the *top* of a chart, compared to when the same data points were positioned near the *bottom* of a chart.

One possible explanation for this finding is that participants made simple associations between absolute position and magnitude, equating physically higher data points with larger magnitudes and physically lower data points with smaller magnitudes. This relates to well-established conceptual metaphors for magnitude, where greater vertical positions denote greater magnitudes [@tversky1997].

An alternative explanation is that participants used the y-axis as a frame of reference for assessing the magnitude of plotted values. For example, when considering data points at the near the bottom of the axis, participants may have recognized the potential for values larger than those observed, consequently associating plotted values with smaller magnitudes.

E1 does not provide a means of differentiating these competing explanations. Drawing inferences from data points' absolute positions would bias magnitude judgments in the same direction as drawing inferences from their relative positions. A high magnitude is implied by a data point's high physical position *and* its superior position in the context other of presented values. Therefore, an additional experiment is required in order to distinguish between the two competing explanations.

Inverting a vertical axis changes the relationship between physical position and numerical value: increasingly *lower* positions represent increasingly *higher* numerical values. This means data points presented near the *bottom* of a chart are numerically *larger* than the accompanying y-axis values. This is illustrated in @fig-rationale. Therefore, inferences invoking relative numerical position would elicit a bias magnitude judgments in the opposite direction compared to inferences invoking data points' physical positions.

In E2, we manipulate data points' physical positions by changing axis limits (as in E1), but employ inverted axes. This allows us to identify the mechanism responsible for the previously observed bias in magnitude judgments. Use of absolute position would be indicated by higher magnitude ratings for data points at *high* physical positions (mirroring the finding for conventional charts). Alternatively, use of relative numerical position would be indicated by higher magnitude ratings for data points at *low* physical positions (the reverse of the finding for conventional charts).

Previous research suggests that charts with inverted axes can be prone to misinterpretation when viewers are not informed about the inversion [@pandey2015; @woodin2022]. Therefore, we provided explicit instruction to ensure participants were aware that inverted charts were presented.

```{r}
#| label: fig-rationale
#| cache: false
#| include: true
#| out-width: "300px"
#| out-height: "23%"
#| fig-asp: 0.7
#| fig.cap: "Rationale for Experiment 2: Distinguishing the Roles of Absolute and Relative Position. In charts with conventional axis orientations (left column), there is congruity between data points’ absolute positions and their relative positions in the chart. In charts with inverted axis orientations (right column), there is incongruity between data points’ absolute positions and their relative positions in the chart. For example, at high absolute positions in conventional charts (top left), data points are relatively higher than implied alternatives. But at the same absolute positions, in inverted charts, the same values are relatively lower than alternatives (top right)."
# visualizing the similarities/differences between absolute and relative position in conventional and inverted charts

# create my theme
my_theme <- function() {
  theme_minimal(base_size = 8) + # originally 12
    theme(panel.border = element_rect(fill = NA, linewidth = 1),
          panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          axis.title.x = element_blank(),
          axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.title.y = element_text(size = 10, face = "bold"),
          axis.text.y = element_text(size = 15, colour = "black", face = "bold"),
          plot.title = element_text(face = "bold"),
          aspect.ratio = 4.5/3,
    )
}

# generate example data
x <- c('A','B','C')
y <- c(38, 38.2, 37.8)

df <- as_tibble(cbind(x, y)) %>%
  mutate_at(vars("y"), as.numeric)

data_mean <- 38

# dataframe with four possible combinations of conditions
id <- expand_grid(c("hi", "lo"), c("conventional", "inverted"))

create_plot <- function(pos, orientation){

  # set upper and lower limits around the population mean of the data, depending on conditions
  lower_lim <- case_when(pos == "lo" & orientation == "conventional" ~ data_mean - 1.5, 
                         pos == "hi" & orientation == "conventional" ~ data_mean - 8.5,
                         pos == "lo" & orientation == "inverted" ~ data_mean - 8.5, 
                         pos == "hi" & orientation == "inverted" ~ data_mean - 1.5)
  upper_lim <- case_when(pos == "lo" & orientation == "conventional" ~ data_mean + 8.5, 
                         pos == "hi" & orientation == "conventional" ~ data_mean + 1.5,
                         pos == "lo" & orientation == "inverted" ~ data_mean + 1.5, 
                         pos == "hi" & orientation == "inverted" ~ data_mean + 8.5)
  
  # set the values for other variables
  
  # y_order = the order for the two y-axis value labels (bottom, top)
  y_order <- case_when(orientation == "conventional" ~ c(lower_lim, upper_lim),
                       orientation == "inverted" ~ c(upper_lim, lower_lim))
  
  # axis_transform = reverse axis or not
  axis_transform <- case_when(orientation == "conventional" ~ "identity",
                              orientation == "inverted" ~ "reverse")
  
  # background colour
  fill_colour <- case_when(orientation == "conventional" ~ "white",
                           orientation == "inverted" ~ "grey")
  
  # opacity of dividing line
  divider_alpha <- case_when(orientation == "conventional" ~ 1,
                           orientation == "inverted" ~ 0)
  
  # text which states whether values are higher or lower than implied alternatives
  comparison_text <- case_when(pos == "lo" & orientation == "conventional" ~ "LOWER", 
                               pos == "hi" & orientation == "conventional" ~ "HIGHER",
                               pos == "lo" & orientation == "inverted" ~ "HIGHER", 
                               pos == "hi" & orientation == "inverted" ~ "LOWER")
  
  # position of comparison text in between data points and farthest limits
  text_pos <- min(upper_lim, lower_lim) + 
    (max(upper_lim, lower_lim) - min(upper_lim, lower_lim))/2

  # start and end points for direction arrow at the side
  arrow_start <- lower_lim +2.5
  arrow_end <- upper_lim -2.5
  
  # create the plot
  g <- df %>% ggplot(aes(x = x,
                         y = y)) + 
    geom_point(size = 4) + 
    ylab("") +
    coord_cartesian(ylim = y_order, 
                    xlim = c(0.5, 3.5), 
                    clip = "off",
                    expand = FALSE) +
    geom_segment(aes(x = -0.21, xend = -0.21, # specifying the vertical arrow
                     y = arrow_start,
                     yend = arrow_end),
                 arrow = arrow(length = unit(0.5,"cm")), size = 1.2, colour = "black") +
        geom_segment(aes(x = -1, xend = -1, 
                     y = -Inf,
                     yend = Inf), alpha = divider_alpha, size = 1) + 
    geom_label(aes(x = 2, y = text_pos), label = paste("Plotted data\n", comparison_text, "than\nalternatives"), fill = "lightgrey") + 
    scale_y_continuous(breaks = seq(lower_lim + 1.5, # breaks where the y-axis labels will be
                                    upper_lim - 1.5,
                                    by = 7),
                       labels = percent_format(scale = 1, accuracy = 1),
                       trans = axis_transform) + # y-axis labels
    my_theme() +
    theme(plot.background = element_rect (fill = fill_colour, color = 'black')) 
  
  
  assign(value = g, envir = .GlobalEnv, paste0("g", 
                                               substr(pos, 1, 1),
                                               substr(orientation, 1, 1)))
  
}

# run the above function for every combination of conditions
invisible(do.call(mapply, c(create_plot, unname(id))))

# add all four plots together, with addition y-axis labels
ghc +  ggtitle('Conventional') + ylab("HIGH\nPhysical Position\n\n") +
  ghi + ggtitle('Inverted') + glc + ylab("LOW\nPhysical Position\n\n") + gli 

```

## Method

### Materials

Materials were identical to E1, except for the inversion of the y-axis in all charts, including practice trials.

### Procedure

The experiment used PsychoPy version 2021.2.3. One slide in the instructions explained to participants how charts with inverted axes function: *"In all graphs in this experiment, the arrow on the 'Chance' axis points downwards, meaning the numbers get bigger as the axis goes down."*. Otherwise, the procedure was identical to E1.

### Design

As in E1, we employed a repeated-measures, within-participants design.

### Participants

A viral social media post on 24th July 2021 endorsing the Prolific.co attracted many new users from a narrow demographic, skewing studies' participant distributions [@charalambides2021] . Therefore, the experiment was not advertised to users who signed-up to Prolific.co after 24th July 2021, or to those who had participated in E1.

```{r}
#| label: E2_demographics
#| cache: false
# extract age data
age_E2 <- distinct(E2_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(age_textbox.text, na.rm = TRUE), sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract gender data
gender_E2 <- distinct(E2_tidy, participant, .keep_all = TRUE) %>% group_by(gender_slider.response) %>% summarise(perc = n()/nrow(.)*100) %>% pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract education data
edu_E2 <- distinct(E2_tidy, participant, .keep_all = TRUE) %>% group_by(edu_slider.response) %>% summarise(perc = n()/nrow(.)*100) %>% filter(edu_slider.response != 'No formal qualications' & edu_slider.response != 'Don\'t know / not applicable') %>% tally(perc)

# extract literacy data
literacy_E2 <- distinct(E2_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(literacy), sd = sd(literacy))

# extract timing data
time_E2 <- distinct(E2_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(time_taken), sd = sd(time_taken))
```

Data were returned by 161 participants. Ten participants' submissions were rejected because they answered more than two of 10 attention check questions incorrectly. One additional participant was excluded from the final dataset because they exceeded the maximum completion time (87 minutes). This left a total of 150 participants whose submissions were used for analysis: (`r printnum(gender_E2$M)`% male, `r printnum(gender_E2$F)`% female). Mean age was `r printnum(age_E2$mean)` (*SD* = `r printnum(age_E2$sd)`)[^3]. `r printnum(edu_E2$n, digits = 0)`% had completed at least secondary education. The mean graph literacy score was `r printnum(literacy_E2$mean)` (*SD* = `r printnum(literacy_E2$sd)`). Participants whose submissions were approved were paid £3.45, and average completion time was `r printnum(time_E2$mean, digits = 0)` minutes.

[^3]: Age data were unavailable for two participants, but was available for all other participants in the dataset.

## Analysis

We used the same analysis methods as in E1.

## Results

### Magnitude Ratings

```{r}
#| label: fig-E2-c
#| cache: false
#| include: true
#| out-width: "300px"
#| fig.cap: "Participants rated the chance of each negative event occurring on a 7-point Likert scale. The distribution of ratings, ranging from \"Very unlikely\" (far left, dark green) to \"Very likely\" (far right, red), is shown separately for charts where values were presented at a high physical position (top) and a low physical position (bottom). Note that data points at high physical positions elicited a larger proportion of ratings on the left-hand side (which represents smaller magnitudes), compared to data points at low physical positions, which elicited a larger proportion of ratings on the right-hand side (representing greater magnitudes)."
# create likert plot for E2 data - chance (magnitude) ratings
L2 <- likert_plot(E2_tidy) +
  annotate("segment", x=400, y=3.2, xend=2700, yend=3.2,
           arrow=arrow(ends = "both", type = "closed", length = unit(10, "pt"))) +
  labs(title = "Experiment 2:\nRatings of Data Points' Magnitudes")

E2_prop <- E2_tidy %>%
  group_by(condition, chance_slider.response) %>%
  summarize(freq = n(),
            raw = round(n()/3000*100, 0),
            perc = paste0(round(n()/3000*100, 0), "%")) %>% ungroup() %>%
  filter(raw > 3)

L2 <- L2 + geom_text(
  data=E2_prop,
  aes(x=freq, label=(perc), group=condition),
  position = position_stack(reverse = TRUE, vjust = 0.5),
  color="black", size=3.5
)

L2
```

```{r}
#| label: E2-c
# E1 chance (magnitude) rating model
E2_c <- buildclmm(chance_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = E2_tidy)
```

```{r}
#| label: E2-c-cmpr
# E1 chance (magnitude) rating model without fixed effect
E2_c_cmpr <- clmm(comparison(E2_c),
             data = E2_tidy)
```

```{r}
#| label: E2-c-anova
#| cache: false
# extract anova results for E1 chance (magnitude) rating models
anova_results(E2_c, E2_c_cmpr)

# summary output for E1 chance (magnitude) rating model
summary_extract(E2_c, "condition1")
```

```{r}
#| label: E2-c-str
#| cache: false
#| include: true
# print table showing model random effects structure (not used in final document)
add_header_above(kable(random_str(E2_c), "latex"), "E2_c")
```

@fig-E2-c plots the distribution of participants' ratings of data points' magnitudes, for data points presented at high and low physical positions. A likelihood ratio test reveals that a model including physical position as a fixed effect explains significantly more variability in ratings than a model which does not include physical position as a fixed effect ($\chi^2$(`r in_paren(E2_c.df)`) = `r printnum(E2_c.LR)`, p `r printp(E2_c.p, add_equals = TRUE)`). Data points' magnitudes were rated as *smaller* when those data points were presented at high physical positions, compared to when the same data points were presented at low physical positions (z = `r printnum(abs(E2_c.statistic))`, p `r printp(E2_c.p.value, add_equals = TRUE)`). 

The odds ratio for the difference between conditions is `r printnum(1/exp(E2_c.estimate))`, `r print_confint(1/exp(E2_c.CI))`. Participants were `r round(1/exp(E2_c.estimate), 1)` times more likely to award a higher magnitude rating to data points presented at high positions than data points presented at low positions. This is equivalent to a Cohen's d value of `r printnum(oddsratio_to_d(1/exp(E2_c.estimate)))`. 

This model employed random intercepts for each scenario. Estimated marginal means are plotted in @fig-E2-c-emm.

```{r}
#| label: fig-E2-c-emm
#| cache: false
#| include: true
#| fig.cap: Estimated marginal means for ratings of data points' magnitudes (generated by the cumulative-link mixed model). Magnitudes were rated as greater when data points in inverted charts were presented at low physical positions. Translucent bars show 95% confidence intervals.
# extract estimated marginal means for E2 chance (magnitude) ratings
E2_c_emm <- emmeans(E2_c@model, ~ condition) %>% as_tibble()

# select the second colour in my_palette for this plot
hex_inverted <- my_palette[2]

# generate plot of estimated marginal means for E3 chance (magnitude) ratings in the two conditions
E2_c_emm %>%
  as_tibble() %>%
  mutate_at(vars("emmean":"asymp.UCL"), as.numeric) %>%
  ggplot(aes(x = condition, y = emmean, colour = I(hex_inverted), group = 1)) +
  geom_linerange(aes(ymin = asymp.LCL, ymax = asymp.UCL),
                 position = position_dodge(width = 0.1),
                 size = 3, alpha = 0.5) +
  geom_point(position = position_dodge(width = 0.1), size = 3) +
  geom_line(position = position_dodge(width = 0.1),
            size = 2) +
  lims(y = c(-1.8, 2)) +
  labs(y = "Estimated\nMarginal Mean",
       x = "Physical Position",
       title = "Experiment 2:\nRatings of Data Points' Magnitudes (Modeled)") +
  scale_x_discrete(labels = c('Low','High'),
                   limits = c("lo", "hi")) +
  theme_minimal(base_size = 18) +
  theme(legend.position = "none",
        panel.grid = element_blank(),
        axis.text.y = element_blank(),
        plot.title = element_text(size=18, hjust = 0.5))
```

### Severity Ratings

```{r}
#| label: E2-s
# E2 severity rating model
E2_s <- buildclmm(severity_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = E2_tidy)
```

```{r}
#| label: E2-s-cmpr
# E2 severity rating model without fixed effect
E2_s_cmpr <- clmm(comparison(E2_s),
             data = E2_tidy)
```

```{r}
#| label: E2-s-anova
#| cache: false
# extract anova results for E1 severity rating models
anova_results(E2_s, E2_s_cmpr)

# summary output for E1 severity rating model
summary_extract(E2_s, "condition1")
```

```{r}
#| label: E2-s-str
#| cache: false
#| include: true
# print table showing model random effects structure (not used in final document)
add_header_above(kable(random_str(E2_s), "latex"), "E2_s")
```

For ratings of the severity of consequences, a likelihood ratio test reveals that a model including physical position as a fixed effect explains significantly more variability in ratings than a model which does not include condition as a fixed effect: ($\chi^2$(`r in_paren(E2_s.df)`) = `r printnum(E2_s.LR)`, p `r printp(E2_s.p, add_equals = TRUE)`). The severity of consequences was rated as greater when data points representing the chance of an event occurring were presented at high physical positions, compared to when the same data points were presented at low physical positions (z = `r printnum(abs(E2_s.statistic))`, p `r printp(E2_s.p.value, add_equals = TRUE)`). 

The odds ratio for the difference between conditions is `r printnum(1/exp(E2_s.estimate))`, `r print_confint(1/exp(E2_s.CI))`. Participants were `r round(1/exp(E2_s.estimate), 1)` times more likely to award a higher severity rating to data points presented at high positions than data points presented at low positions. This is equivalent to a Cohen's d value of `r printnum(oddsratio_to_d(1/exp(E2_s.estimate)))`. 

This model employed random intercepts for each scenario, plus random intercepts and by-position slopes for each participant.

### Data Visualisation Literacy

```{r}
#| label: E2-cl
# generate E2 chance (magnitude) rating model with literacy as additional fixed effect
E2_cl <- clmm(add.terms(formula(E2_c), "literacy"),
              data = E1_tidy)
```

```{r}
#| label: E2-sl
# generate E2 severity rating model with literacy as additional fixed effect
E2_sl <- clmm(add.terms(formula(E2_s), "literacy"),
              data = E2_tidy)
```

```{r}
#| label: E2-lit-summary
#| cache: false
# summary output for E2 chance (magnitude) rating model with literacy
summary_extract(E2_cl, "condition1")

# summary output for E2 severity rating model with literacy
summary_extract(E2_sl, "condition1")
```

We also generated two additional models, to test whether or not the above results could be explained by differences in graph literacy. These models were identical to the above models except for the inclusion of participants' graph literacy scores as an additional fixed effect. Adjusting for participants' graph literacy scores did not change the pattern of results regarding ratings of the magnitude of data points themselves (z = `r printnum(abs(E2_cl.statistic))`, p `r printp(E2_cl.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E2_cl.estimate))`, `r print_confint(1/exp(E2_cl.CI))`) or severity of consequences (z = `r printnum(abs(E2_sl.statistic))`, p `r printp(E2_sl.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E2_sl.estimate))`, `r print_confint(1/exp(E2_sl.CI))`).

### Interaction Analysis

```{r}
#| label: combine datasets
#| cache: false

comb_tidy <- rbind(
  E1_tidy %>% cbind(orientation = "conventional"),
  E2_tidy %>% cbind(orientation = "inverted") %>% 
    select(-edu_slider.response) %>%
    mutate(participant = as.numeric(participant) + 150)) %>%
  mutate(orientation = factor(orientation))

contrasts(comb_tidy$condition) <- matrix(c(.5, -.5))
contrasts(comb_tidy$orientation) <- matrix(c(.5, -.5))
```

```{r}
#| label: int-c
int_c <- buildclmm(chance_slider.response ~ condition*orientation +
                    (1 + condition | participant) +
                    (1 + condition*orientation | item_no),
                  data = comb_tidy)
```

```{r}
#| label: int-c-cmpr
# interaction model without interaction fixed effect
int_c_cmpr <- clmm(comparison(int_c),
             data = comb_tidy)
```

```{r}
#| label: int-c-anova
#| cache: false
# extract anova results for interaction models
anova_results(int_c, int_c_cmpr)

# summary output for interaction model
summary_extract(int_c, "orientation1:condition1")

# pairwise comparisons
int_c_emm <- emmeans(int_c@model, ~ orientation * condition)
```

```{r}
#| label: fig-int
#| cache: false
int_c_emm %>%
  as_tibble() %>%
  mutate_at(vars("emmean":"asymp.UCL"), as.numeric) %>%
  ggplot(aes(x = condition, y = emmean, colour = orientation)) +
  geom_linerange(aes(ymin = asymp.LCL, ymax = asymp.UCL), 
                 position = position_dodge(width = 0.1),
                 size = 3, alpha = 0.5) +
  geom_point(position = position_dodge(width = 0.1), size = 3) +
  geom_line(aes(group = orientation), 
            position = position_dodge(width = 0.1), 
            size = 2) +
  lims(y = c(-1.8, 2)) + 
  labs(y = "Estimated\nMarginal Mean",
       x = "Physical Position",
       colour = "Orientation:",
       title = "Experiment 1 & 2:\nInteraction in Ratings of Data Points' Magnitudes (Modeled)") +
  scale_x_discrete(labels = c('Low','High'),
                   limits = c("lo", "hi")) +
  #scale_colour_manual(labels = c('Conventional', 'Inverted'),
  #                    limits = c("conventional", "inverted"),
  #                    values = my_palette) + 
  theme_minimal(base_size = 18) +
  theme(legend.position = "top",
        panel.grid = element_blank(),
        axis.text.y = element_blank(),
        plot.title = element_text(size=16, hjust = 0.5))
```


# General Discussion

Given the use of data visualization for reliable communication of numerical information, understanding how design choices affect interpretations is an important matter. Through a pair of experiments, we demonstrate that judgments of data points' magnitudes are influenced by a chart's axis limits. These experiments provide insight into the cognitive processes involved in assessing magnitudes in data visualisations. We manipulated the range of values on the axis accompanying plotted data, which affected the numerical context in which data appeared and the physical position of data points. However, regardless of their physical positions, data points were associated with greater magnitudes when they appeared close to end of the axis associated with higher values. This highlights viewers' sensitivity to the framing of values presented in charts.

Interpretation of *the same* absolute value is biased by its relative position. The impact of surrounding information on assessments of data is an example of a framing effect. We illustrate that this effect occurs in the absence of contrasting data points: the presence of blank space is sufficient for informing magnitude judgments.

## Relationship to Prior Work

The present data complement findings from research on y-axis truncation, which has observed that axis limits accompanying plotted values can influence viewers' impressions of those values. While previous investigations have shown that *comparisons* of plotted values are affected by y-axis limits [@pandey2015; @correll2020a ; @witt2019; @yang2021], the present findings show that they also influence *magnitude judgments*. This finding supports the notion that viewers are sensitive to visualization rhetoric, which involves provoking a specific interpretation through a particular presentation of numerical information [@hullman2011].

A previous study addressing a similar question also concluded that a data point's location within a range of values affects interpretation of its magnitude @sandman1994. The present study builds upon this research by identifying the mechanism behind this effect and removing the confound of variable axes ranges. It also extends the finding beyond a single scenario (asbestos) to a wider range of situations. By analyzing different types of judgment separately, rather than using a combined measure, we verify that axis limits affect interpretations of the specific variable displayed in a chart.
